#!/usr/bin/env bun
/**
 * ğŸ¤– LLM Benchmark â€” Model KarÅŸÄ±laÅŸtÄ±rmasÄ±
 *
 * En kritik darboÄŸaz: LLM TTFT (Time To First Token)
 * FarklÄ± modeller + :nitro varyantlarÄ± + prompt boyutu optimizasyonu test edilir.
 */

import { ENV, fmt, printResults, avg, median, type TimingResult } from "./utils";

// â”€â”€â”€ Test edilecek modeller â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const MODELS = [
  // Mevcut
  "google/gemini-2.5-flash",
  // Nitro varyant (en hÄ±zlÄ± provider'a yÃ¶nlendir)
  "google/gemini-2.5-flash:nitro",
  // Daha hafif Gemini
  "google/gemini-2.0-flash-001",
  // KÃ¼Ã§Ã¼k + hÄ±zlÄ± modeller
  "google/gemini-2.0-flash-lite-001",
  // GPT-4o Mini
  "openai/gpt-4o-mini",
  // Claude Haiku (ultra-hÄ±zlÄ±)
  "anthropic/claude-3-haiku",
  // Llama kÃ¼Ã§Ã¼k + nitro
  "meta-llama/llama-3.1-8b-instruct:nitro",
  // Mistral Small
  "mistralai/mistral-small-3.1-24b-instruct",
];

// â”€â”€â”€ System prompts (kÄ±sa vs uzun) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const SHORT_SYSTEM_PROMPT = `Sen Wingman, TÃ¼rkÃ§e teknik mÃ¼lakatÃ§Ä±sÄ±n. KÄ±sa ve Ã¶z cevap ver, 2-3 cÃ¼mle. Sesli konuÅŸma formatÄ±nda yaz.`;

const LONG_SYSTEM_PROMPT = `Sen Wingman adÄ±nda deneyimli bir teknik mÃ¼lakatÃ§Ä±sÄ±n. TÃ¼rkÃ§e konuÅŸuyorsun.
AdayÄ± profesyonel ama samimi bir ÅŸekilde karÅŸÄ±la. SorularÄ±nÄ± net ve anlaÅŸÄ±lÄ±r sor.
CevaplarÄ± deÄŸerlendirirken yapÄ±cÄ± ol. KÄ±sa ve Ã¶z konuÅŸ â€” her cevabÄ±n 2-3 cÃ¼mleyi geÃ§mesin.
Bu sesli bir konuÅŸmadÄ±r. Liste yapma, madde madde yazma. DoÄŸal ve akÄ±cÄ± konuÅŸ.
Her cÃ¼mleni tamamla, yarÄ±da bÄ±rakma. Teknik terimleri TÃ¼rkÃ§e aÃ§Ä±kla.`;

const USER_MESSAGE = "Hash map ve array arasÄ±ndaki farkÄ± aÃ§Ä±klayabilir misin? Hangi durumda hangisini tercih edersin?";

// â”€â”€â”€ Single model benchmark â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function benchModel(
  model: string,
  systemPrompt: string,
  maxTokens: number,
): Promise<TimingResult & { tokens: number; firstTokenContent: string }> {
  const messages = [
    { role: "system", content: systemPrompt },
    { role: "user", content: USER_MESSAGE },
  ];

  const start = performance.now();
  let ttfb = 0;
  let tokens = 0;
  let fullText = "";
  let firstTokenContent = "";

  try {
    const response = await fetch("https://openrouter.ai/api/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${ENV.OPENROUTER_API_KEY}`,
        "Content-Type": "application/json",
        "HTTP-Referer": ENV.SITE_URL,
        "X-Title": "Wingman Benchmark",
      },
      body: JSON.stringify({
        model,
        messages,
        stream: true,
        max_tokens: maxTokens,
        temperature: 0.7,
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      return {
        label: model,
        ttfb: -1,
        total: -1,
        tokens: 0,
        firstTokenContent: `ERROR: ${response.status} - ${errorText.slice(0, 100)}`,
      };
    }

    const reader = response.body?.getReader();
    if (!reader) throw new Error("No stream");

    const decoder = new TextDecoder();

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const chunk = decoder.decode(value, { stream: true });
      const lines = chunk.split("\n");

      for (const line of lines) {
        if (!line.startsWith("data: ")) continue;
        const data = line.slice(6).trim();
        if (data === "[DONE]") continue;

        try {
          const parsed = JSON.parse(data);
          const token = parsed.choices?.[0]?.delta?.content;
          if (!token) continue;

          tokens++;
          fullText += token;

          if (ttfb === 0) {
            ttfb = performance.now() - start;
            firstTokenContent = token;
          }
        } catch {
          // skip
        }
      }
    }
  } catch (err) {
    return {
      label: model,
      ttfb: -1,
      total: -1,
      tokens: 0,
      firstTokenContent: `NETWORK ERROR: ${err}`,
    };
  }

  const total = performance.now() - start;
  const tokPerSec = tokens / (total / 1000);

  return {
    label: model,
    ttfb,
    total,
    tokens,
    firstTokenContent,
    extra: {
      tokens,
      "tok/s": Math.round(tokPerSec),
      responseLength: fullText.length,
    },
  };
}

// â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function main() {
  console.log("ğŸ¤– LLM Benchmark â€” Model KarÅŸÄ±laÅŸtÄ±rmasÄ±");
  console.log("â”€".repeat(60));

  const RUNS = 2; // Her model iÃ§in 2 run (ilk Ã§aÄŸrÄ± cold olabilir)

  // â”€â”€ Test 1: Model karÅŸÄ±laÅŸtÄ±rmasÄ± (kÄ±sa prompt, max_tokens=200) â”€â”€
  console.log("\nğŸ“Š Test 1: Model TTFT KarÅŸÄ±laÅŸtÄ±rmasÄ± (short prompt, max_tokens=200)");
  console.log(`   Her model ${RUNS}x Ã§alÄ±ÅŸtÄ±rÄ±lacak...\n`);

  const modelResults: TimingResult[] = [];

  for (const model of MODELS) {
    process.stdout.write(`   Testing ${model}... `);
    const runs: { ttfb: number; total: number; tokens: number }[] = [];

    for (let i = 0; i < RUNS; i++) {
      const r = await benchModel(model, SHORT_SYSTEM_PROMPT, 200);
      if (r.ttfb > 0) {
        runs.push({ ttfb: r.ttfb, total: r.total, tokens: r.tokens });
      }
      if (i === 0 && r.ttfb < 0) {
        console.log(`âŒ ${r.firstTokenContent}`);
        break;
      }
    }

    if (runs.length > 0) {
      const result: TimingResult = {
        label: model,
        ttfb: median(runs.map((r) => r.ttfb)),
        total: median(runs.map((r) => r.total)),
        extra: {
          tokens: Math.round(avg(runs.map((r) => r.tokens))),
          "tok/s": Math.round(avg(runs.map((r) => r.tokens)) / (avg(runs.map((r) => r.total)) / 1000)),
          runs: runs.length,
        },
      };
      modelResults.push(result);
      console.log(`TTFB: ${fmt(result.ttfb)}, Total: ${fmt(result.total)}`);
    }

    // Rate limit korumasÄ±
    await Bun.sleep(500);
  }

  printResults("Model TTFT KarÅŸÄ±laÅŸtÄ±rmasÄ± (short prompt, 200 tokens)", modelResults);

  // â”€â”€ Test 2: Prompt boyutu etkisi (en hÄ±zlÄ± model ile) â”€â”€
  const fastestModel = modelResults.sort((a, b) => a.ttfb - b.ttfb)[0];
  if (!fastestModel) {
    console.log("âŒ HiÃ§bir model Ã§alÄ±ÅŸmadÄ±!");
    return;
  }

  console.log(`\nğŸ“Š Test 2: Prompt Boyutu Etkisi (${fastestModel.label})`);

  const promptResults: TimingResult[] = [];

  // KÄ±sa prompt
  process.stdout.write("   Short prompt... ");
  const shortR = await benchModel(fastestModel.label, SHORT_SYSTEM_PROMPT, 200);
  if (shortR.ttfb > 0) {
    promptResults.push({ label: "Short System Prompt (120 char)", ttfb: shortR.ttfb, total: shortR.total });
    console.log(`TTFB: ${fmt(shortR.ttfb)}`);
  }

  await Bun.sleep(300);

  // Uzun prompt
  process.stdout.write("   Long prompt... ");
  const longR = await benchModel(fastestModel.label, LONG_SYSTEM_PROMPT, 200);
  if (longR.ttfb > 0) {
    promptResults.push({ label: "Long System Prompt (450 char)", ttfb: longR.ttfb, total: longR.total });
    console.log(`TTFB: ${fmt(longR.ttfb)}`);
  }

  await Bun.sleep(300);

  // Ã‡ok uzun prompt (gerÃ§ek senaryoyu simÃ¼le et)
  const VERY_LONG_PROMPT = LONG_SYSTEM_PROMPT + `\n\n[MÃ¼lakata atanan problem]
BaÅŸlÄ±k: Two Sum
Zorluk: Easy
AÃ§Ä±klama: Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.
Ä°lgili Konular: Array, Hash Table
Beklenen Zaman KarmaÅŸÄ±klÄ±ÄŸÄ±: O(n)

[Aday Ã–zgeÃ§miÅŸ]
Ä°sim: Test User
Pozisyon: Frontend Developer
Deneyim: 3 yÄ±l
Yetenekler: React, TypeScript, Node.js, Python

--- MÃœLAKAT TALÄ°MATLARI ---
Bu bir teknik mÃ¼lakat sorusudur. Sen mÃ¼lakatÃ§Ä±sÄ±n, aday bu problemi Ã§Ã¶zmeye Ã§alÄ±ÅŸacak.
1. Problemi doÄŸal bir dille aÃ§Ä±kla.
2. AdayÄ±n yaklaÅŸÄ±mÄ±nÄ± sor, direkt Ã§Ã¶zÃ¼m verme.
3. Test sonuÃ§larÄ±nÄ± deÄŸerlendir.`;

  process.stdout.write("   Very long prompt (real scenario)... ");
  const veryLongR = await benchModel(fastestModel.label, VERY_LONG_PROMPT, 200);
  if (veryLongR.ttfb > 0) {
    promptResults.push({ label: "Very Long Prompt (real scenario, ~900 char)", ttfb: veryLongR.ttfb, total: veryLongR.total });
    console.log(`TTFB: ${fmt(veryLongR.ttfb)}`);
  }

  printResults("Prompt Boyutu Etkisi", promptResults);

  // â”€â”€ Test 3: max_tokens etkisi â”€â”€
  console.log(`\nğŸ“Š Test 3: max_tokens Etkisi (${fastestModel.label})`);

  const tokenResults: TimingResult[] = [];

  for (const maxTok of [100, 200, 300, 500]) {
    process.stdout.write(`   max_tokens=${maxTok}... `);
    const r = await benchModel(fastestModel.label, SHORT_SYSTEM_PROMPT, maxTok);
    if (r.ttfb > 0) {
      tokenResults.push({
        label: `max_tokens=${maxTok}`,
        ttfb: r.ttfb,
        total: r.total,
        extra: { tokens: r.tokens },
      });
      console.log(`TTFB: ${fmt(r.ttfb)}, Total: ${fmt(r.total)}, Tokens: ${r.tokens}`);
    }
    await Bun.sleep(300);
  }

  printResults("max_tokens Etkisi", tokenResults);

  // â”€â”€ Ã–zet â”€â”€
  console.log("\n" + "â•".repeat(60));
  console.log("  ğŸ“‹ Ã–ZET Ã–NERÄ°LER");
  console.log("â•".repeat(60));

  const best = modelResults[0];
  if (best) {
    console.log(`  1. Model: ${best.label} (TTFB: ${fmt(best.ttfb)})`);
  }
  console.log(`  2. :nitro suffix kullan â†’ en hÄ±zlÄ± provider'a yÃ¶nlendir`);
  console.log(`  3. System prompt'u kÄ±sa tut â†’ TTFB'yi dÃ¼ÅŸÃ¼rÃ¼r`);
  console.log(`  4. max_tokens=150-200 yeterli â†’ gereksiz token Ã¼retme`);
  console.log();
}

main().catch(console.error);
