# ğŸï¸ Benchmark: Interview Pipeline Latency Optimization

## Hedef: Toplam E2E Latency < 2 saniye

Mevcut pipeline:
```
User Ses â†’ [STT] â†’ [LLM] â†’ [TTS] â†’ AI Ses
           ~800ms   ~1500ms   ~500ms   = ~2800ms (ortalama)
```

## ğŸ“Š Benchmark Scriptleri

| Script | AÃ§Ä±klama |
|--------|----------|
| `bench-stt.ts` | STT endpoint latency (Freya STT) |
| `bench-llm.ts` | LLM model karÅŸÄ±laÅŸtÄ±rmasÄ± (TTFT + throughput) |
| `bench-tts.ts` | TTS yÃ¶ntem karÅŸÄ±laÅŸtÄ±rmasÄ± (stream vs generate vs fetch) |
| `bench-e2e.ts` | Tam pipeline E2E benchmark |
| `bench-parallel.ts` | Paralel/speculative pipeline deneyleri |
| `run-all.ts` | TÃ¼m benchmark'larÄ± Ã§alÄ±ÅŸtÄ±r, sonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r |

## ğŸš€ KullanÄ±m

```bash
cd benchmark
bun run bench-stt.ts        # Sadece STT
bun run bench-llm.ts        # Sadece LLM
bun run bench-tts.ts        # Sadece TTS
bun run bench-e2e.ts        # Full E2E
bun run bench-parallel.ts   # Paralel pipeline deneyleri
bun run run-all.ts          # Hepsini Ã§alÄ±ÅŸtÄ±r
```

## ğŸ§ª Test Edilen Optimizasyon Stratejileri

### 1. ğŸ¤– LLM Model SeÃ§imi (En bÃ¼yÃ¼k fark burada!)
- `google/gemini-2.5-flash` (mevcut) â€” iyi ama TTFT yÃ¼ksek olabilir
- `google/gemini-2.5-flash:nitro` â€” throughput optimize
- `google/gemini-2.0-flash-001` â€” daha hafif, daha hÄ±zlÄ± TTFT
- `anthropic/claude-3-haiku` â€” ultra-hÄ±zlÄ±, kÃ¼Ã§Ã¼k model
- `meta-llama/llama-3.1-8b-instruct:nitro` â€” kÃ¼Ã§Ã¼k + nitro
- `openai/gpt-4o-mini` â€” hÄ±zlÄ± ve ucuz
- `mistralai/mistral-small-3.1-24b-instruct` â€” iyi denge

### 2. ğŸ¤ STT OptimizasyonlarÄ±
- Audio chunk boyutunu azalt (daha kÄ±sa kayÄ±t)
- VAD (Voice Activity Detection) ile sessizlik tespiti
- Audio format: webm/opus â†’ daha kÃ¼Ã§Ã¼k dosya boyutu
- Audio sample rate dÃ¼ÅŸÃ¼rme (16kHz yeterli)

### 3. ğŸ”Š TTS OptimizasyonlarÄ±
- `fal.stream("/stream")` â€” gerÃ§ek zamanlÄ± PCM16 streaming (EN HIZLI)
- `fetch("/audio/speech")` â€” tek seferde binary
- `fal.subscribe("/generate")` â€” kuyruk + CDN URL
- Speed parametresi: 1.0 â†’ 1.15 (daha hÄ±zlÄ± konuÅŸma = daha kÄ±sa audio)

### 4. âš¡ Pipeline OptimizasyonlarÄ±
- **Sentence-level interleaving** (mevcut) â€” LLM cÃ¼mle bitince hemen TTS baÅŸlat
- **Sub-sentence chunking** â€” VirgÃ¼lle bile TTS baÅŸlat (riskli ama hÄ±zlÄ±)
- **Speculative TTS** â€” Ä°lk birkaÃ§ kelimeyi tahmin edip Ã¶nceden ses Ã¼ret
- **Context window pruning** â€” Conversation history'yi kÄ±salt â†’ daha hÄ±zlÄ± LLM
- **System prompt compression** â€” Prompt'u kÄ±salt â†’ TTFT dÃ¼ÅŸer
- **max_tokens limiti** â€” 500 â†’ 200 (daha kÄ±sa yanÄ±t = daha hÄ±zlÄ±)
- **Streaming SSE parsing optimize** â€” Daha verimli token okuma

### 5. ğŸŒ Network OptimizasyonlarÄ±
- **Connection pooling** â€” HTTP keep-alive
- **DNS pre-resolve** â€” fal.run ve openrouter.ai iÃ§in
- `:nitro` suffix â€” OpenRouter'da en hÄ±zlÄ± provider'a yÃ¶nlendir
- **Regional endpoint** â€” fal.ai'da en yakÄ±n region

### 6. ğŸ§  AkÄ±llÄ± KÄ±sayollar
- **Greeting cache** â€” Ä°lk selamlama mesajÄ± Ã¶nceden Ã¼retilmiÅŸ ses
- **Common response cache** â€” SÄ±k yanÄ±tlar iÃ§in TTS cache
- **Parallel STT+LLM prefetch** â€” STT bitmeden LLM'e "hazÄ±rlan" sinyali

---

## ğŸ“ˆ Hedef Metrikler

| Metrik | Mevcut | Hedef |
|--------|--------|-------|
| STT Latency | ~800ms | < 500ms |
| LLM TTFT | ~1500ms | < 600ms |
| TTS TTFB | ~500ms | < 300ms |
| **Toplam E2E** | **~2800ms** | **< 1400ms** |
| First Audio Byte | ~2500ms | < 1200ms |
